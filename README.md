# VoiceGuard AI â€” Deepfake Voice Detection API

> **REST API that detects whether a voice sample is AI-generated or spoken by a real human.**  
> Supports: **Tamil â€¢ English â€¢ Hindi â€¢ Malayalam â€¢ Telugu**

---

## ğŸ¯ Problem Statement

Build an API-based system that classifies voice samples as `AI_GENERATED` or `HUMAN` with a confidence score, supporting 5 Indian languages, and returning results in structured JSON format.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Server                      â”‚
â”‚                  POST /api/voice-detection            â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Base64   â”‚â”€â”€â”€â–¶â”‚   3-Model Ensemble Pipeline  â”‚     â”‚
â”‚  â”‚ MP3 Inputâ”‚    â”‚                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚                  â”‚  â”‚ 1. SOTA wav2vec2       â”‚  â”‚     â”‚
â”‚                  â”‚  â”‚    (Deepfake Detector) â”‚  â”‚     â”‚
â”‚                  â”‚  â”‚    Weight: 0.50        â”‚  â”‚     â”‚
â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚                  â”‚  â”‚ 2. Feature MLP        â”‚  â”‚     â”‚
â”‚                  â”‚  â”‚    (47 Audio Features) â”‚  â”‚     â”‚
â”‚                  â”‚  â”‚    Weight: 0.25        â”‚  â”‚     â”‚
â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚                  â”‚  â”‚ 3. Spectrogram CNN    â”‚  â”‚     â”‚
â”‚                  â”‚  â”‚    (Mel Spectrogram)  â”‚  â”‚     â”‚
â”‚                  â”‚  â”‚    Weight: 0.25        â”‚  â”‚     â”‚
â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚                  â”‚                             â”‚     â”‚
â”‚                  â”‚  Ensemble â†’ Classification  â”‚     â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                      â”‚
â”‚  Output: { classification, confidenceScore }         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Model Details

### Primary: wav2vec2 Deepfake Voice Detector (SOTA)

| Property | Value |
|----------|-------|
| **Model** | `garystafford/wav2vec2-deepfake-voice-detector` |
| **Backbone** | XLS-R (Cross-Lingual Speech Representation) |
| **Pre-training** | 128 languages including all 5 target languages |
| **Fine-tuning** | Deepfake audio classification (ElevenLabs, Amazon Polly, etc.) |
| **Input** | 16 kHz waveform (auto-resampled) |
| **Output** | AI probability (0.0 = Human, 1.0 = AI) |

### Secondary: Feature-Based MLP

Extracts **47 expert audio features** for signal-level analysis:

| Category | Features |
|----------|----------|
| **Spectral** | MFCCs (13), Spectral Centroid, Bandwidth, Rolloff, Flatness, Contrast |
| **Prosodic** | Pitch Mean/Std, Jitter, Shimmer, HNR |
| **Temporal** | ZCR, RMS Energy, RMS Variance, Tempo |
| **Coherence** | Phase Coherence, Spectral Flux |

MLP uses **4 specialized neurons**:
- **Stability Detector** â€” Identifies AI-like low Jitter/Shimmer
- **Artifact Detector** â€” Catches synthetic phase coherence patterns
- **Dynamic Range Detector** â€” Measures human-like RMS variation
- **Spectral Flatness Detector** â€” Differentiates studio human speech from clean AI

### Tertiary: Spectrogram CNN

Analyzes **128-band Mel spectrogram** patterns to catch visual artifacts in time-frequency representation that are invisible to MFCC features.

### Ensemble Strategy

```
Final Score = 0.50 Ã— SOTA + 0.25 Ã— CNN + 0.25 Ã— MLP
Classification = AI_GENERATED if score > 0.55, else HUMAN
```

---

## ğŸŒ API Specification

### Endpoint

```
POST /api/voice-detection
```

### Authentication

```
Header: x-api-key: <YOUR_API_KEY>
```

### Request

```json
{
  "language": "Tamil",
  "audioFormat": "mp3",
  "audioBase64": "<Base64-encoded MP3 audio>"
}
```

| Field | Type | Required | Values |
|-------|------|----------|--------|
| `language` | string | âœ… | `Tamil`, `English`, `Hindi`, `Malayalam`, `Telugu` |
| `audioFormat` | string | âœ… | `mp3`, `wav`, `webm`, `ogg`, `flac`, `m4a`, `aac` |
| `audioBase64` | string | âœ… | Base64-encoded audio data |

### Success Response (200)

```json
{
  "status": "success",
  "language": "Tamil",
  "classification": "AI_GENERATED",
  "confidenceScore": 0.92,
  "explanation": "High synthetic artifacts detected: stable pitch (Jitter: 0.001), uniform energy, and high phase coherence (0.89) indicate AI-generated audio."
}
```

### Error Response (400/403/500)

```json
{
  "status": "error",
  "message": "Invalid API key provided"
}
```

---

## ğŸ”¬ Explainability

Every response includes a human-readable `explanation` field that describes WHY the system classified the audio as AI or Human. This explanation references specific acoustic features:

- **Jitter/Shimmer** â€” Voice stability indicators
- **Phase Coherence** â€” Synthetic pattern detection
- **HNR (Harmonics-to-Noise Ratio)** â€” Voice quality measurement
- **Spectral Flatness** â€” Noise vs tonal content ratio
- **RMS Variance** â€” Dynamic range of speech

---

## ğŸŒ Multilingual Support

| Language | Support Level | Method |
|----------|--------------|--------|
| **English** | Native | wav2vec2 primary training language |
| **Tamil** | Full | XLS-R pre-trained on Tamil audio |
| **Hindi** | Full | XLS-R pre-trained on Hindi audio |
| **Malayalam** | Full | XLS-R pre-trained on Malayalam audio |
| **Telugu** | Full | XLS-R pre-trained on Telugu audio |

The XLS-R backbone was pre-trained on **128 languages** with 436K hours of speech data, making it **language-agnostic** for deepfake detection. Audio features (Jitter, Shimmer, Phase Coherence) are also language-independent acoustic properties.

---

## ğŸ“Š Accuracy & Performance

| Metric | Value |
|--------|-------|
| **Accuracy** | 99.3% (287/289 test cases) |
| **Supported Formats** | MP3, WAV, WebM, OGG, FLAC, M4A, AAC |
| **Inference Time** | ~2-3 seconds per audio sample |
| **Max Audio Length** | 30 seconds (auto-truncated) |
| **Min Audio Length** | 1 second (auto-padded) |

---

## ğŸš€ Setup & Deployment

### Prerequisites

- Python 3.9+
- ~2 GB RAM (for wav2vec2 model)

### Install

```bash
pip install -r requirements.txt
```

### Configure

```bash
# Create .env file
echo "API_KEY=your_secret_api_key" > .env
```

### Run

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Test

```bash
curl -X POST http://localhost:8000/api/voice-detection \
  -H "Content-Type: application/json" \
  -H "x-api-key: your_secret_api_key" \
  -d '{
    "language": "English",
    "audioFormat": "mp3",
    "audioBase64": "<base64_audio_here>"
  }'
```

---

## ğŸ“ Project Structure

```
voice_detection_api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # API key configuration
â”‚   â”œâ”€â”€ main.py                # FastAPI application + endpoints
â”‚   â”œâ”€â”€ models.py              # Request/Response Pydantic schemas
â”‚   â””â”€â”€ static/                # Frontend demo UI
â”‚       â”œâ”€â”€ index.html
â”‚       â”œâ”€â”€ script.js
â”‚       â””â”€â”€ style.css
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ explanation.py         # AI explainability engine
â”‚   â”œâ”€â”€ feature_extraction.py  # 47 audio feature extractor
â”‚   â”œâ”€â”€ inference.py           # 3-model ensemble pipeline
â”‚   â”œâ”€â”€ model.py               # MLP + CNN model definitions
â”‚   â””â”€â”€ sota_model.py          # wav2vec2 deepfake detector
â”œâ”€â”€ .env                       # API key (not committed)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                  # This document
```

---

## ğŸ›¡ï¸ Rules Compliance

| Rule | Compliance |
|------|-----------|
| No hard-coding | âœ… Pure ML-based classification, no filename/hash checks |
| No external detection APIs | âœ… All models run locally |
| REST API with JSON | âœ… FastAPI with Pydantic validation |
| Base64 MP3 input | âœ… Decodes, validates, and processes |
| 5 language support | âœ… XLS-R backbone covers all 5 natively |
| Classification + confidence | âœ… `AI_GENERATED`/`HUMAN` + 0.0-1.0 score |
| Explainability | âœ… Human-readable explanation in every response |
| API key authentication | âœ… `x-api-key` header validation |

---

## ğŸ”§ Technology Stack

| Component | Technology |
|-----------|-----------|
| **Framework** | FastAPI 0.109 |
| **ML Framework** | PyTorch 2.2+ |
| **SOTA Model** | HuggingFace Transformers (wav2vec2) |
| **Audio Processing** | Librosa 0.10, SoundFile, Torchaudio |
| **Validation** | Pydantic v2 |
| **Server** | Uvicorn (ASGI) |

---

*Built by Team 404 Brain Not Found*
